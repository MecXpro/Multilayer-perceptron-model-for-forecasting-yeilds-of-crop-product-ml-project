# Multilayer-perceptron-model-for-forecasting-yeilds-of-crop-product-ml-project
1.Performance: 
The performance evaluation of the predictive model constructed in this coursework involved applying various distance and correlation metrics on the dataset processed by the implemented Python code. Specifically, the metrics employed were Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Pearson correlation coefficient, all calculated using Python's scikit-learn library.
Mean Absolute Error (MAE) offered an intuitive quantification of the on average magnitude of errors in predictions without any consideration of direction. It was computed using the scikit-learn function mean_absolute_error(y_true, y_pred) where the actual yield is y_true and the predicted yield y_pred. Formally, the formula for the calculation is:
MAE =(∑|y_i-y ̂_i  |)/n
Mean Squared Error (MSE) stressed on large errors because each error term was squared, and as such, there was a clear display of the impact of outliers. This metric was calculated using the scikit-learn function mean_squared_error(y_true, y_pred), and Mathematically as:
MSE=  (∑(y_(i- ) (y_i ) ̂)^2)/n
To further interpret magnitude of errors in units that are consistent with that of the target variable, Root Mean Squared Error (RMSE) was obtained by taking the square root of the value of MSE. This stage gave a readily interpretable index of the error of the prediction.
Furthermore, the Pearson correlation coefficient assessed the strength of the linear relationship between the real and predicted yield values revealing the consistency with which the forecasts followed the actual yields. This coefficient has been calculated based on the function from NumPy which is np.corrcoef(y_true, y_pred)[0, 1], according to the formula: 
r=(∑▒〖(y_(i  )- y ̅)(y ̂_i-y ̂  ̅)〗)/(√(∑▒〖(y_i- y ̅)^2  ∑▒(〗) y ̂_i- y ̂  ̅)^2 )
where ȳ and ŷ̄ represent respectively, the average of actual and predicted yields. 
The dataset included 5000 data samples. To train and validate the model successfully, common practice was followed and data was split in 80-20 ratio, giving 4000 instances for training and 1000 instances for validation. This splitting was conducted using the train_test_split function of scikit-learn making sure that partitioning of the data is random but repeatable by fixing the random state (random_state=42). 
This structured divide enabled serious testing of the model’s capacity for generalization by looking at its performance on unseen data thus ensuring an objective measure free of bias. The training subset was used only for fitting models and tuning the hyperparameters, whereas the testing subset was reserved solely to determine the predictive accuracy of the models. 
 
Figure 1: Scatter plot of predicted vs. actual crop yields
In figure 1, The graphic representation of “Predicted vs Actual Yield” plot created using matplotlib gave a graphical representation to visually inspect prediction accuracy. The diagonal dashed line (y = x) is the line of perfect prediction accuracy, and the scatter points show deviations from the ideal. By observation from this plot it was evident that there was clustering of predictions although there were some distinct outliers. These deviations produced a visual representation of the strengths and weaknesses of the predictive accuracy. 
The models comprehensive numerical metrics along with visual diagnostics from a scatter plot generated in python provide solid model performance evaluation. These assessments combined served to validate the model’s predictive ability while pointing to obvious directions for possible future improvements, e.g. fine turn of feature selection, or moving to more advanced modeling, or more robust data preprocessing. 
Finally, the following explicit computation of metrics and detailed description of the data splitting approach executed through Python coding gives clarity and rigor to this performance evaluation, allowing confident interpretation of the predictive reliability and effectiveness of the developed model.
2. Model:
The predictive model developed for this project is a Multi-layer perceptron (MLP) regressor, a supervised learning algorithm that can model complex nonlinear relationship between input and continuous output. The interest in the use of the MLP regressor was first and foremost driven by the fact that it is flexible, with low computational needs; additionally, it has proven to perform well on a variety of regression problems when applied to high-dimensional data. Implementation of MLP regressor was performed using Python’s scikit-learn library with usage of MLPRegressor class. The model comprises multiple layers: an input layer matching the number of input features, several hidden layers, and an output layer making out continuous predictions. After several preliminary runs the hyperparameters for the final model of the MLP were carefully selected to improve its prediction performance. Specifically, the optimized hyperparameters included: 
• Hidden layer sizes: Specified as (100, 50) hence two hidden layers, the first with 100 neurons and the other with 50 neurons. Selection of these values was dependent on their performance in a grid search validation exercise. 
• Activation function: The efficiencies and abilities of the ReLU to curtail vanishing gradients and enable rapid convergence made it the preferred choice of input activation. 
• Solver: The Adam optimization algorithm was adopted given the ability of its adaptive rate to facilitate model convergence better, and much faster, than other solvers, including stochastic gradient descent, which has been a widely used solver of such kind of problems. 
• Learning rate: Set to ‘adaptive’, which lets the learning rate to adapt dynamically depending on the training process, aiding the model to converge more effectively. 
• Maximum iterations: Limited to 1000 because only this number will allow the training sufficient without being overburdensome for the computer specifying the best tradeoff between model complexity and computational tractability. 
• Random state: 42 but fixed to ensure reproducibility of results. 
The performance of hyperparameter optimization was done methodically using cross validation using grid search (GridSearchCV) using 5-fold cross validation. 
This technique precisely investigated combinations of hyperparameters, and the hyperparameter settings for which the prediction errors were the lowest according to scores from cross-validation were picked. This organized stance had contributed to the idealization of the choice of model parameters and increased the model’s predictive performance over unseen data. 
In order to rigorously avoid overfitting and improve the generality of the model, multiple purposeful measures were used during training and optimization of the model. First, we split the dataset into training and validation sets to keep the model’s performance under evaluation on the data the model had never seen before. Secondly, cross validation when implementing hyper parameters tuning had the effect of containing the danger of the model fitting too close to a certain subsets of data.
In addition, regularization techniques were applied in the form of an L2 penalty, also called weight decay, implied due to the default values done by the Adam solver used in the scikit-learn implementation. This discouraged over complex model parameters and made learning more general. 
Another important strategy that was used to avoid overfitting was early stopping. The training of the model was terminated a specified number of iterations (n_iter_no_change=10) without significant improvement of the validation score. This method saved the model from redundant training beyond the stage at which improvement no longer occurred, therefore maintaining generalized performance as opposed to overfitting the model to training set. 
In addition, feature scaling was carefully done to ensure that there was uniformity as well as stability in training. StandardScaler class of scikit-learn was used to scale data for every feature having zero mean and unit variance, a great improvement in the training projective and a better stability and accelerate convergence of MLP regressor. 
To conclude, the proposed MLP regression model, adjusted with tuned hyperparameters and regularizations, achieved a desirable compromise between predictive accuracy and generalization. Careful choices regarding the parameters of the model as well as the sensible implementation of cross-validation and early stopping significantly attenuated the possible danger of overfitting, with the result that strong predictive power in new unseen data was achieved.
3. Feature and Labels:
The successful implementation and correctness of any prediction model largely depend on strategic choice and clear definition of the input features and output labels. For this project, pertinent features and labels were selected from the given dataset, defined in the Python implementation and explained in terms of their relevance to the forecast of agricultural yields. 
The input to train the multilayer perceptron model (MLP) consisted of a combination of climate, soil, and historical agricultural data that was extracted straight from given data set. In particular, input features included such parameters as average annual rainfall, temperature, soil moisture, soil acidity (pH), and data on yield in previous years. Each of these variables was retrieved from the CSV files of the initial dataset, using Python’s pandas library where they were loaded, loaded and organized into structured dataframes. Data processing involved the choice of the relevant columns based on domain knowledge and experience in prior empirical exercises, after which the data cleaning operations of imputation of missing values, a normalization of the features followed. 
Average annual rainfall and temperature were chosen because they strongly affect the productivity of crops, by regulating important biological processes in plants. These meteorological variable were calculated on an annual average from the monthly variables available in the given dataset. Similarly agricultural soil moisture and pH levels were also included due to their critical role in plant growth and availability of nutrients. Historical yields turned out to be an added context that has become important indicators of the prevailing trends and allowed the model to introduce temporal dependence to the predictions of yields. 
Normalization of each feature was performed using the StandardScaler from scikit-learn with zero mean-unit variance. This pretreatment step would guarantee that all features would make equal contribution towards model learning and also prevent any particular variable from overwhelming prediction outcome to the detriment of others because of scale differences.
The output data, or label, used for this predictive model was clearly the “crop yield” which was measured annually and formed the key target for the forecasting model. The label was taken literally from the dataset, extracted along with other features in the pandas dataframes and extracted into a separate vector for distinction between dependent variables (label) and the independent variables (features). 
The logic in the explicit definition of crop yield as the label was based on the objective of the project, namely: accurate prediction of future agricultural production. Therefore, the extraction of yield as the prediction target was perfectly matching the project’s forecasting purpose. On the other hand, the assignment of environmental and historical variables as inputs enabled the model to discover and understand complex relationships between these influential factors and yields in agriculture.
 In addition, the dataset was split skillfully to maintain historical data integrity. Historical yields that served as input features were lagged over the target yields by one year. For instance, yield data for year t was used as input feature to predict yield for year t+1, clearly enforcing temporal consistency and logical predictive relationship within this dataset. This close alignment strengthened the model’s predictive validity, because the forecasting task overtly involved the predicting a year into the future. 
Summing up, the features and label chosen and extracted for the purpose of this project were logically-based, domain-informed, with a direct link to the project’s objective of forecasting of yields of crops. The transparent extraction, normalization, and temporal alignment of input features, as well as clean and explicit demarcation of the output label, set forth a strict regime necessary for the successful prediction performance of the developed MLP predictive model.


4. Preprocessing:
Preprocessing is a key step in the machine learning process alone; which in turn correlate to the quality and accuracy of the predictive model. In this project, the steps of data preprocessing where well and systematically adopted in various steps spelled out explicitly in the Python code and operated carefully in a structured manner. These preprocessing actions were critical in making the dataset ready for use for the multilayer perceptron (MLP) model to understand and learn from the data as well. 
First, to import, raw dataset was imported using pandas (python library). This process involved loading data from CSV files into structured dataframes that made following manipulation and analysis possible. In this first step, the data was subjected to exploratory examination, with the aim of determining the presence of missing values, anomalies and inconsistencies that might threaten the integrity and reliability of the predictive model.
 After running the load of initial data, missing value treatment was carried out as an important step of preprocessing. Lacking data points were handled through imputation strategies and the pandas standard library provided functionalities for such analysis. In particular, numerical columns with missing values were replaced with the column average values to make the dataset complete without introducing much bias. The reason why the method of mean imputation was selected was its simplicity and its ability to retain distribution and consistency in numerical data, which was relevant particularly when proportion of value absent was not high. 
After the imputation step, standard scaling was performed using the StandardScaler module of the scikit-learn library. Normalizing scaling features to zero mean and unit variance standardized the dataset greatly enhancing model convergence and stability. Such standardization prevented any one feature with inherently larger magnitudes from unreasonably dominating the learning of the model. Thus, the neural network was able to successfully optimize parameters without the danger of some variables overwhelming the learning process by virtue of their original scale.
The temporal alignment of historical yield data was another preprocessing measure explicitly addressed in the Python implementation. Since the goal of the project involved projecting yield for the next year, historical yield values were one year different relative to the output label. Such temporal shifting, thus, made it possible for historical yield data of year t to be used as an input feature to make yield predictions for year t+1, therefore making the logical make-up and the chronological arrangement of the data consistent. Furthermore, the categorical variables contained in the original dataset were not left untouched, their processing was carried out as a result of preprocessing using encoding methods. In particular, categorical variables such as "region type" were transformed into their numeric representation off the back of a one-hot encoding from pandas’ get_dummies option. This encoding was imperative in enabling categorical information to be of use to the MLP regressor that intrinsically needs a numerical input for optimal modeling. The data set partitioning done under the preprocessing phase entailed the partition of the processed data into training and testing subset with 80-20 split ratio. This structured division happened through the use of the train_test_split function of scikit-learn, which supported a clear demarcation between the training dataset for training of model and dataset only for the model evaluation. Keeping this division was important for unbiased estimation of the model's generalization performance. Together, these preprocessing activities greatly improved the predictive ability of the MLP model by addressing data quality problem, normalizing scale of feature, respecting logical temporal relations and treating categorical variables correctly. The detail and discipline with which these pre-processing steps were carried out, as described clearly in the python implementation, became the solid backbone to a robust, reliable, and predictive analytical model that was accurate and stable and generalizable, for forecasting crop yields.
